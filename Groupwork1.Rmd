---
title: "Wage Prediction Using Machine Learning and AutoML"
author: "Your Name"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# 1. Load Required Libraries

We start by loading in the necessary libraries. These libraries are for data exploration machine learning autoML and some require extra installations. For autoML you need to have the 64 bit version of JAVA.
```{r libraries}
# Install missing packages
packages <- c("caret", "randomForest", "pdp", "ggplot2", "dplyr", "caretEnsemble", "h2o", "data.table")
to_install <- packages[!packages %in% installed.packages()[, "Package"]]
if(length(to_install)) install.packages(to_install)

# H2O special handling
if ("package:h2o" %in% search()) detach("package:h2o", unload=TRUE)
if ("h2o" %in% rownames(installed.packages())) remove.packages("h2o")
install.packages(c("RCurl","jsonlite"))
install.packages("h2o", repos = "https://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")

# Load libraries
library(h2o)
library(caret)
library(caretEnsemble)
library(randomForest)
library(pdp)
library(ggplot2)
library(dplyr)
library(data.table)
```

# 2. Load the Data

We load a preprocessed dataset data_wage.RData which contains the wages and all the features that we will need for the prediction.
```{r data-load}

load("data_wage.RData")
df <- data
```

# 3. Data Exploration & Preprocessing

We will first take a look at our data, we can see that our data exists out of more than 10 thousand rows and 78 features. We then check for missing values, and we can see that there are none, if there were any missing values, we would remove the row.
```{r data-explore}
str(df)
summary(df)

# Missing data summary
missing_summary <- sapply(df, function(x) sum(is.na(x)))
print(missing_summary)

# Remove rows with missing values
df <- na.omit(df)
```

When exploring the data, some weird data arrised, 18-21 year olds earining extremely high amounts? Are these outliers? Should we remove them? Let's take a look, first we take everyone that earns, while the features people ussually look at would dispove that Like for example, in our case is it believable that somebody that is 18-21 or for somebody that has less that 3 years of experience to earn more than 100 thousand? Probably not. So that what we took a look at. We gave everybody that earns more than 100k and that has less than 3 years of expereince or that is younger than 25 a possible outlier flag. We did this so it was easier to keep track of these individuals.

```{r data-explore}
# Outlier flags
df$outlier_flag <- with(df, 
                        (wage > 100000 & (years_experience %in% c("0-1", "1-2") | age %in% c("18-21", "22-24")))
)
```
First I thought it might had something to do with if they used ML at work. The easiest way to see if my hypothesis is right, is to disprove it. So I gave everyone that uses ML at work earns less than 50 thousand and is younger than 25 or has less than 3 years of experience a flag to disprove my hypothesis. Now if there was almost nobody that had this new flag then we could conclude that ML at work does in fact have something to do with it.
```{r data-explore}

df$disporve_outlier_flag <- with(df, 
                                 (wage < 50000 & (years_experience %in% c("0-1", "1-2") & age %in% c("18-21", "22-24") & ML_atwork %in% c("We have well established ML methods (i.e., models in production for more than 2 years)","We recently started using ML methods (i.e., models in production for less than 2 years)")))
)

```

Sadly this wasn't the case, so we hadn't found a reason yet why some of these very young or inexperienced people earned so much. Time to do some further analysis. There are 78 features and over 10 thousand rows. With just looking at the data will be hard to find these features that are the cause of these high earners. 

```{r data-explore}
df_18_21 <- subset(df, age == "18-21")
df_18_21$income_group <- ifelse(df_18_21$wage >= 50000, "50k+", "<50k")
df_18_21$income_group <- factor(df_18_21$income_group, levels = c("<50k", "50k+"))
features_to_plot <- names(df_18_21)[!(names(df_18_21) %in% c("wage", "income_group", "outlier_flag", "disporve_outlier_flag"))]
cat_features <- features_to_plot[sapply(df_18_21[features_to_plot], function(x) is.character(x) || is.factor(x))]

# Loop through and plot each categorical feature
for (feature in cat_features) {
  p <- ggplot(df_18_21, aes_string(x = feature, fill = "income_group")) +
    geom_bar(position = "fill") +
    scale_y_continuous(labels = scales::percent_format()) +
    labs(title = paste("Income Group Proportions by", feature),
         x = feature,
         y = "Proportion",
         fill = "Income Group") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(p)
}



```

From this we could see some interesting statistics, just check out the plots, percent actively coding, How long have you been writing code to analyze data, and for how many years have you used machine learning methods at work or in school. Other graphs like the one "Do you consider yourself to be a data scientist" will not help us at all, as every column has around the same amount of people who earn a lot, making this noise across all categories for that feature.


A normal question is should we consider gender as an important feature? While gender used to heavily affect the wage, does it still to this day and should we include this? Is this ethical?

```{r is gender an important feature?}
varImpPlot(rf_model)
ggplot(df, aes(x = gender, y = wage)) +
  geom_boxplot(fill = "lightblue", outlier.shape = NA) +
  coord_cartesian(ylim = c(0, quantile(df$wage, 0.95))) +  # optional: cap extreme outliers
  labs(
    title = "Wage Distribution by Gender Across Industries",
    y = "Wage",
    x = "Gender"
  ) +
  facet_wrap(~ industry, scales = "free_y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```
Gender includes Female, Male, Prefer not to say, and Prefer to self-describe. The difference between the median of all these is very minor, so you might think we could let this variable out. But if we look closer, per industry for example, we see that females on average earn a lot more than males, if we would let the feature gender go and we would try to predict a female or male for the industry broadcasting, it would give a very wrong answer. So even though it might not be ethical, we do have to leave the feature in.

Convert categorical variables to factors:

Categorical variables are converted to factors to prepare them for modeling.


```{r factor-conversion}
df$gender <- as.factor(df$gender)
df$education <- as.factor(df$education)
df$country <- as.factor(df$country)
df$age <- as.factor(df$age)
df$years_experience <- as.factor(df$years_experience)
df$job_role <- as.factor(df$job_role)
df$industry <- as.factor(df$industry)
df$For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school.. <- as.factor(df$For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school..)
df$How.long.have.you.been.writing.code.to.analyze.data. <- as.factor(df$How.long.have.you.been.writing.code.to.analyze.data.)
```

# 4. Feature Selection and Dummy Encoding

We select key predictors and apply dummy encoding to convert categorical features to numeric format for ML modeling.

```{r dummy-encoding}
model_data <- df %>% 
  select(wage, age, years_experience, education, gender, country, job_role, industry, How.long.have.you.been.writing.code.to.analyze.data., For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school..)

dummy_model <- caret::dummyVars(~ ., data = model_data[,-1])
dummy_data <- predict(dummy_model, newdata = model_data[,-1])
model_matrix <- data.frame(wage = model_data$wage, dummy_data)
```

# 5. Train-Test Split

```{r train-test-split}
set.seed(123)
train_index <- createDataPartition(model_matrix$wage, p = 0.7, list = FALSE)
train_data <- model_matrix[train_index, ]
test_data  <- model_matrix[-train_index, ]
```

# 6. AutoML with H2O

```{r automl}
h2o.init()
h2o.xgboost.available()

df_h2o <- as.h2o(model_matrix)
set.seed(12)
splits <- h2o.splitFrame(df_h2o, ratios = 0.8, seed = 1234)
train <- splits[[1]]
valid <- splits[[2]]

dep_var <- "wage"
indep_vars <- setdiff(colnames(df_h2o), dep_var)

automl <- h2o.automl(
  x = indep_vars,
  y = dep_var,
  training_frame = train,
  leaderboard_frame = valid,
  max_models = 10,
  seed = 12,
  sort_metric = "RMSE",
  exclude_algos = c("XGBoost")
)

lb <- automl@leaderboard
print(lb)

best_model <- automl@leader
print(best_model)
```

Evaluate the best model:

```{r automl-eval}
best_rmse <- h2o.get_best_model(automl, criterion = "RMSE")
pred_best_rmse <- h2o.predict(best_rmse, valid)
predictions <- as.data.table(pred_best_rmse)
perf_best_rmse <- h2o.performance(best_rmse, valid)

performance_single <- function(perf_object) {
  rmse <- h2o.rmse(perf_object)
  mse <- h2o.mse(perf_object)
  mae <- h2o.mae(perf_object)
  r2  <- h2o.r2(perf_object)
  cat("Performance Metrics:
")
  cat("---------------------
")
  cat(sprintf("RMSE: %.2f
", rmse))
  cat(sprintf("MSE : %.2f
", mse))
  cat(sprintf("MAE : %.2f
", mae))
  cat(sprintf("R²  : %.4f
", r2))
}

single_best_rmse <- performance_single(perf_best_rmse)

# Extract predictions and actuals
pred_df <- as.data.frame(h2o.cbind(pred_best_rmse, valid$wage))
colnames(pred_df) <- c("predicted", "actual")
```

Plot residuals and predicted vs actual values:

```{r automl-plots, fig.width=6, fig.height=4}
# Residual plot
ggplot(pred_df, aes(x = actual, y = predicted - actual)) +
  geom_point(alpha = 0.4) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs. Actual Wage", x = "Actual Wage", y = "Residuals")

# Predicted vs actual plot
ggplot(pred_df, aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, color = "blue", linetype = "dashed") +
  labs(title = "Predicted vs. Actual Wage", x = "Actual Wage", y = "Predicted Wage")
```

# 6.2. AutoML Explainability

```{r automl-explain}
exp_automl <- h2o.explain(automl, valid)
print(exp_automl)

StackedEnsemble_rsme <- h2o.get_best_model(automl, algorithm = "StackedEnsemble", criterion = "rmse")
GBM_rsme <- h2o.get_best_model(automl, algorithm = "GBM", criterion = "rmse")

exp_GBM <- h2o.explain(GBM_rsme, valid)
print(exp_GBM)

h2o.varimp_plot(GBM_rsme)
```

# 7. Random Forest Model Development

```{r rf-model}
set.seed(123)
rf_model <- randomForest(wage ~ ., data = train_data, importance = TRUE)
print(rf_model)
```

# 8. Model Evaluation

```{r rf-eval}
predictions <- predict(rf_model, test_data)
rmse_value <- sqrt(mean((test_data$wage - predictions)^2))
rsq_value <- cor(test_data$wage, predictions)^2

cat("Test RMSE:", rmse_value, "
")
cat("Test R-squared:", rsq_value, "
")
```

# 9. Explainability: Feature Importance & Partial Dependence

```{r rf-explain}
importance_values <- importance(rf_model)
print(importance_values)
varImpPlot(rf_model, main = "Variable Importance for Wage Prediction")

pdp_experience <- partial(rf_model, pred.var = "years_experience", train = train_data)
plotPartial(pdp_experience, main = "Partial Dependence of Wage on Years of Experience",
            xlab = "Years of Experience", ylab = "Predicted Wage")
```

# 10. Real-world Application: Predicting Team Member Wages

```{r predict-team}
team_raw <- data.frame(
  age = factor(c("30-34", "35-39", "22-24"), levels = levels(df$age)),
  years_experience = factor(c("5-11", "5-11", "0-1"), levels = levels(df$years_experience)),
  education = factor(c("Master’s degree", "Doctoral degree", "Bachelor’s degree"), levels = levels(df$education)),
  gender = factor(c("Female", "Male", "Male"), levels = levels(df$gender)),
  country = factor(c("United States of America", "United States of America", "Switzerland"), 
                   levels = levels(df$country)),
  job_role = factor(c("Data Scientist", "Software Engineer", "Student"), levels = levels(df$job_role)),
  industry = factor(c("Computers/Technology", "Computers/Technology", "I am a student"), levels = levels(df$industry))
)

team_dummy <- predict(dummy_model, newdata = team_raw)
team_matrix <- data.frame(team_raw, team_dummy)
team_matrix$predicted_wage <- predict(rf_model, newdata = team_matrix)

team_matrix[, c("age", "years_experience", "education", "gender", "country", "job_role", "industry", "predicted_wage")]
```

# 11. Save Model and Results

```{r save-results, eval=FALSE}
save(rf_model, file = "rf_wage_model.RData")
write.csv(team_matrix, file = "team_wage_predictions.csv", row.names = FALSE)
```

# End of Report
