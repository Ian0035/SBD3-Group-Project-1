---
title: "Wage Prediction Using Machine Learning and AutoML"
author: "Your Name"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# 1. Load Required Libraries

We start by loading in the necessary libraries. These libraries are for data exploration machine learning autoML and some require extra installations. For autoML you need to have the 64 bit version of JAVA.
```{r libraries}
# Install missing packages
packages <- c("caret", "randomForest", "pdp", "ggplot2", "dplyr", "caretEnsemble", "h2o", "data.table")
to_install <- packages[!packages %in% installed.packages()[, "Package"]]
if(length(to_install)) install.packages(to_install)


# Load libraries
library(h2o)
library(caret)
library(caretEnsemble)
library(randomForest)
library(pdp)
library(ggplot2)
library(dplyr)
library(data.table)
```

# 2. Load the Data

We load a preprocessed dataset data_wage.RData which contains the wages and all the features that we will need for the prediction.
```{r data-load}

load("data_wage.RData")
df <- data
```

# 3. Data Preprocessing

Goal: Understand the data before modeling.
Why? Data-driven decisions start with exploration.

We will first take a look at our data, we can see that our data exists out of more than 10 thousand rows and 78 features. These features include, age, years of experience, industry job role, if they have used ML before and a whole lot more. We then check for missing values, and we can see that there are none, if there were any missing values, we would remove the row.
```{r data-explore}
str(df)
summary(df)

# Missing data summary
missing_summary <- sapply(df, function(x) sum(is.na(x)))
print(missing_summary)

# Decision: Drop rows with missing data if the proportion is low.
cat("Rows before NA removal:", nrow(df), "\n")
df <- na.omit(df)
cat("Rows after NA removal:", nrow(df), "\n")
```

# 4 Data exploration
Why? To justify feature selection based on data relationships.


While exploring the data, we noticed some unusual cases—specifically, individuals aged 18 to 21 reporting extremely high incomes. This raised the question: are these legitimate data points, or are they outliers?

To investigate, we filtered the data to include only those earning over $100,000. Typically, people this young or with limited experience rarely reach that income level. For instance, it's unlikely that someone under 25 or with less than 3 years of experience would be earning over $100,000.

To help identify potential anomalies, we flagged all individuals meeting these criteria (earning over $100K and either under 25 years old or with less than 3 years of experience) as possible outliers. This made it easier to track and analyze these cases separately.

```{r data-explore}
# Outlier flags
df$outlier_flag <- with(df, 
                        (wage > 100000 & (years_experience %in% c("0-1", "1-2") | age %in% c("18-21", "22-24")))
)
View(df)
```
First I thought it might had something to do with the feature: ML_atwork. The easiest way to see if my hypothesis is right, is to disprove it. So I gave everyone that uses ML at work earns less than 50 thousand and is younger than 25 or has less than 3 years of experience a flag to disprove my hypothesis. Now if there was almost nobody that had this new flag then we could conclude that ML at work does in fact have something to do with it.
```{r data-explore}

df$disporve_outlier_flag <- with(df, 
                                 (wage < 50000 & (years_experience %in% c("0-1", "1-2") & age %in% c("18-21", "22-24") & ML_atwork %in% c("We have well established ML methods (i.e., models in production for more than 2 years)","We recently started using ML methods (i.e., models in production for less than 2 years)")))
)

```

Sadly this wasn't the case, as there are a whole lot of people who earn less than 50k a year who also use ML_atwork. So this feature alone didn't cause the high wages for these young/ barely experienced people. Time to do some further analysis. There are 78 features and over 10 thousand rows. With just looking at the data will be hard to find these features that are the cause of these high earners, that's why we will plot some charts to get a clear look of what these high earners do, or in what category they fall. We will plot charts for every categorical feature. 

```{r data-explore}
df_18_21 <- subset(df, age == "18-21")
df_18_21$income_group <- ifelse(df_18_21$wage >= 50000, "50k+", "<50k")
df_18_21$income_group <- factor(df_18_21$income_group, levels = c("<50k", "50k+"))
features_to_plot <- names(df_18_21)[!(names(df_18_21) %in% c("wage", "income_group", "outlier_flag", "disporve_outlier_flag"))]
cat_features <- features_to_plot[sapply(df_18_21[features_to_plot], function(x) is.character(x) || is.factor(x))]

# Loop through and plot each categorical feature
for (feature in cat_features) {
  p <- ggplot(df_18_21, aes_string(x = feature, fill = "income_group")) +
    geom_bar(position = "fill") +
    scale_y_continuous(labels = scales::percent_format()) +
    labs(title = paste("Income Group Proportions for people between 18 and 21 by", feature),
         x = feature,
         y = "Proportion",
         fill = "Income Group") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(p)
}

```

From this we could see some interesting statistics, just check out the plots, "percent actively coding", "How long have you been writing code to analyze data", and "for how many years have you used machine learning methods at work or in school". We can see a clear trend in these categories, so these are some features we got to keep an eye out for. Other graphs like the one "Do you consider yourself to be a data scientist" will not help us at all, as every column has around the same amount of people who earn a lot, making this noise across all categories for that feature.


A normal question is should we consider gender as an important feature? While gender used to heavily affect the wage, does it still to this day and should we include this? Is this ethical?

```{r is gender an important feature?}
ggplot(df, aes(x = gender, y = wage)) +
  geom_boxplot(fill = "lightblue", outlier.shape = NA) +
  coord_cartesian(ylim = c(0, quantile(df$wage, 0.95))) +  # optional: cap extreme outliers
  labs(
    title = "Wage Distribution by Gender Across Industries",
    y = "Wage",
    x = "Gender"
  ) +
  facet_wrap(~ industry, scales = "free_y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```
Gender includes Female, Male, Prefer not to say, and Prefer to self-describe. The difference between the median of all these is very minor, so you might think we could let this variable out. But if we look closer, per industry for example, we see that females on average earn a lot more than males, if we would let the feature gender go and we would try to predict a female or male for the industry broadcasting, it would give a very wrong answer. So even though it might not be ethical, we do have to leave the feature in.


```{r data-explore}
# Numeric correlation with wage
numeric_vars <- sapply(df, is.numeric)
if (sum(numeric_vars) > 1) {
  cor_wage <- cor(df[, numeric_vars], use = "complete.obs")
  print(cor_wage["wage", ])
}

# Identify all numeric variables (excluding the target variable "wage")
numeric_vars <- setdiff(names(df)[sapply(df, is.numeric)], "wage")

# Loop through each numeric variable and run linear regression
for (var in numeric_vars) {
  formula <- as.formula(paste("wage ~", var))
  lm_result <- lm(formula, data = df)
  
  cat("\nLinear regression of wage on", var, ":\n")
  print(summary(lm_result))
}


# Group means for categorical features
cat_vars <- c("education", "years_experience", "gender", "country", "job_role", "industry", "age")
for (v in cat_vars) {
  print(ggplot(df, aes_string(x = v, y = "wage")) +
          geom_boxplot() +
          ggtitle(paste("Wage by", v)))
}

```
# 5 Data preparation & encoding

Convert categorical variables to factors:

Categorical variables are converted to factors to prepare them for modeling.
```{r factor-conversion}
df$gender <- as.factor(df$gender)
df$education <- as.factor(df$education)
df$country <- as.factor(df$country)
df$age <- as.factor(df$age)
df$years_experience <- as.factor(df$years_experience)
df$job_role <- as.factor(df$job_role)
df$industry <- as.factor(df$industry)
df$ML_atwork <- as.factor(df$ML_atwork)
df$percent_actively.coding <- as.factor(df$percent_actively.coding)
df$For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school.. <- as.factor(df$For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school..)
df$How.long.have.you.been.writing.code.to.analyze.data. <- as.factor(df$How.long.have.you.been.writing.code.to.analyze.data.)
```

# 6 Feature Selection

We select key predictors and that we think are important for our model to predict the right wage. From the data exploration above we chose the features: age, years_experience, education, gender, country, job_role, industry, ML_atwork, percent_actively.coding, How.long.have.you.been.writing.code.to.analyze.data., For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school..

```{r Feature Selection}
model_data <- df %>% 
  select(wage, age, years_experience, education, gender, country, job_role, industry, ML_atwork, percent_actively.coding, How.long.have.you.been.writing.code.to.analyze.data., For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school..)

```

# Dummy Encoding

We apply dummy encoding to convert categorical features to numeric format for autoML modeling.

```{r dummy-encoding}

dummy_model <- caret::dummyVars(~ ., data = model_data[,-1])
dummy_data <- predict(dummy_model, newdata = model_data[,-1])
model_matrix <- data.frame(wage = model_data$wage, dummy_data)
```

# 7. AutoML with H2O

We use H2O’s AutoML to automatically train and tune multiple models. XGBoost is excluded due to compatibility issues with windows.

```{r automl}
h2o.init()

h2o.xgboost.available()

df_h2o <- as.h2o(model_matrix)
set.seed(12)
splits <- h2o.splitFrame(df_h2o, ratios = 0.8, seed = 1234)
train <- splits[[1]]
valid <- splits[[2]]

dep_var <- "wage"
indep_vars <- setdiff(colnames(df_h2o), dep_var)

automl <- h2o.automl(
  x = indep_vars,
  y = dep_var,
  training_frame = train,
  leaderboard_frame = valid,
  max_models = 13,
  seed = 12,
  sort_metric = "RMSE",
  exclude_algos = c("XGBoost")
)

lb <- automl@leaderboard
print(lb)

best_model <- automl@leader
print(best_model)
```

# 8. Random Forest Model Development
We build a traditional Random Forest for comparison.

We split the data into training and testing sets to evaluate model generalization.

We chose Random Forest as a robust, interpretable, and non-parametric model that performs well on high-dimensional, categorical datasets like ours which has over 70 categorical features. It served as a reliable benchmark against AutoML's more complex ensembles while keeping control over the modeling process.

```{r rf-model}
set.seed(123)

train_index <- createDataPartition(model_matrix$wage, p = 0.7, list = FALSE)
train_data <- model_matrix[train_index, ]
test_data  <- model_matrix[-train_index, ]


rf_model <- randomForest(wage ~ ., data = train_data, importance = TRUE)
print(rf_model)
```


# 9 AutoML Explainability

We use H2O’s built-in tools to explore variable importance and local explanations.

```{r automl-explain}

StackedEnsemble_rsme <- h2o.get_best_model(automl, algorithm = "StackedEnsemble", criterion = "rmse")
GBM_rsme <- h2o.get_best_model(automl, algorithm = "GBM", criterion = "rmse")

exp_GBM <- h2o.explain(GBM_rsme, valid)
print(exp_GBM)

h2o.varimp_plot(GBM_rsme)
```

# 10 Random Forest Explainability

We visualize feature importance and partial dependence of wages on experience.

```{r rf-explain}

# For Random Forest
importance_values <- importance(rf_model)
print(importance_values)

varImpPlot(rf_model, main = "Variable Importance for Wage Prediction (RF)")
cat("Top 5 RF features:\n")
print(head(sort(importance(rf_model)[,1], decreasing = TRUE), 5))

# Partial dependence plot for United states
pdp_experience <- partial(rf_model, pred.var = "country.United.States.of.America", train = train_data)
plotPartial(pdp_experience, main = "Partial Dependence: Wage ~ country.United.States.of.America",
            xlab = "country.United.States.of.America", ylab = "Predicted Wage")

```


# 11 AutoML VS our own Random Forest model

```{r rf-eval}
# Performance of AutoML model
perf_best_rmse <- h2o.performance(best_model, valid)
aml_rmse <- h2o.rmse(perf_best_rmse)
aml_mae  <- h2o.mae(perf_best_rmse)
aml_r2   <- h2o.r2(perf_best_rmse)

# Performance of Random Forest (train/test split)
rf_model <- randomForest(wage ~ ., data = train_data, importance = TRUE)
rf_pred <- predict(rf_model, test_data)
rf_rmse <- sqrt(mean((test_data$wage - rf_pred)^2))
rf_mae  <- mean(abs(test_data$wage - rf_pred))
rf_r2   <- cor(test_data$wage, rf_pred)^2

# Results summary table
results_table <- data.frame(
  Model = c("Random Forest", "H2O AutoML"),
  RMSE = c(rf_rmse, aml_rmse),
  MAE  = c(rf_mae, aml_mae),
  R2   = c(rf_r2, aml_r2)
)
print(results_table)

```

# 12 Plot residuals and predicted vs actual values:

We visualize how well the model predicted wages using residual and prediction plots.

```{r automl-plots, fig.width=6, fig.height=4}
# AutoML predictions
pred_best_rmse <- h2o.predict(best_model, valid)
pred_df <- as.data.frame(h2o.cbind(pred_best_rmse, valid$wage))
colnames(pred_df) <- c("predicted", "actual")

# Residuals plot
ggplot(pred_df, aes(x = actual, y = predicted - actual)) +
  geom_point(alpha = 0.4) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs. Actual Wage", x = "Actual Wage", y = "Residuals")

# Predicted vs actual
ggplot(pred_df, aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, color = "blue", linetype = "dashed") +
  labs(title = "Predicted vs. Actual Wage", x = "Actual Wage", y = "Predicted Wage")

```


# 13 Real-world Application: Predicting Team Member Wages

We use the trained model to predict the wages of team members based on their profile.

```{r predict-team}
team_raw <- data.frame(
  age = factor(c("22-24", "22-24", "22-24", "25-29"), levels = levels(df$age)),
  years_experience = factor(c("0-1", "0-1", "3-4", "5-11"), levels = levels(df$years_experience)),
  education = factor(c("Bachelor’s degree", "Bachelor’s degree", "Bachelor’s degree", "Bachelor’s degree"), levels = levels(df$education)),
  gender = factor(c("Male", "Male", "Female", "Female"), levels = levels(df$gender)),
  country = factor(c("Denmark", "Belgium", "Switzerland", "Switzerland"), 
                   levels = levels(df$country)),
  job_role = factor(c("Data Scientist", "Student", "Other", "Other"), levels = levels(df$job_role)),
  industry = factor(c("Computers/Technology", "Computers/Technology", "Computers/Technology", "Computers/Technology"), levels = levels(df$industry)),
  ML_atwork = factor(c("We have well established ML methods (i.e., models in production for more than 2 years)", "We recently started using ML methods (i.e., models in production for less than 2 years)", "I do not know", "We use ML methods for generating insights (but do not put working models into production)"), levels = levels(df$ML_atwork)),
  percent_actively.coding = factor(c("75% to 99% of my time", "25% to 49% of my time", "0% of my time", "0% of my time"), levels = levels(df$percent_actively.coding)),
  For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school.. = factor(c("3-4 years", "3-4 years", "I have never studied machine learning but plan to learn in the future", "< 1 year"), levels = levels(df$For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school..)),
  How.long.have.you.been.writing.code.to.analyze.data. = factor(c("3-5 years", "3-5 years", "< 1 year", "5-10 years"), levels = levels(df$How.long.have.you.been.writing.code.to.analyze.data.))

)

team_dummy <- predict(dummy_model, newdata = team_raw)
team_matrix <- data.frame(team_raw, team_dummy)
# Convert to H2OFrame
team_h2o <- as.h2o(team_matrix)

# Predict and assign
team_matrix$predicted_wage <- as.vector(predict(GBM_rsme, newdata = team_h2o))

team_matrix[, c("age", "years_experience", "education", "gender", "country", "job_role", "industry","ML_atwork", "percent_actively.coding", "For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school..", "How.long.have.you.been.writing.code.to.analyze.data.",  "predicted_wage")]
```
Our results: Predicted wages
Row 1 = Ian: 73822.06
Row 2 = Piet: 26811.76
Row 3 = Rahel: 64331.60
Row 4 = Dana: 97305.01

# A shap plot per person to see why they earn the amount

```{r save-results, eval=FALSE}
library(ggplot2)

# Get SHAP values and dummy features
shap_values <- h2o.predict_contributions(GBM_rsme, team_h2o)
shap_df <- as.data.frame(shap_values)
team_dummy_df <- as.data.frame(team_h2o)

# Loop through each individual
for (i in 1:4) {
  individual_shap <- shap_df[i, ]
  individual_input <- team_dummy_df[i, ]
  
  # Remove BiasTerm
  individual_shap <- individual_shap[, !(names(individual_shap) == "BiasTerm")]
  individual_input <- individual_input[, !(names(individual_input) == "BiasTerm")]
  
  # Find active features (equal to 1)
  active_feature_names <- names(individual_input)[which(individual_input == 1)]
  
  # Filter SHAP values by active feature names
  shap_filtered <- data.frame(
    Feature = active_feature_names,
    SHAP_value = unlist(individual_shap[active_feature_names])
  )
  
  # Sort by absolute SHAP value
  shap_filtered <- shap_filtered[order(abs(shap_filtered$SHAP_value), decreasing = TRUE), ]
  
  # Plot
  p <- ggplot(shap_filtered, aes(x = reorder(Feature, SHAP_value), y = SHAP_value, fill = SHAP_value > 0)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    labs(
      title = paste("SHAP Contributions (Active Features Only) – Individual", i),
      x = "Active Feature",
      y = "SHAP Value"
    ) +
    scale_fill_manual(values = c("TRUE" = "#1f77b4", "FALSE" = "#d62728")) +
    theme_minimal()
  
  
  print(p)
}


```

These shap plots should explain exactly why everyone got their wage but underneath we'll give a more indepth explanation:

The explanation, Ian, Piet, and, Rahel are all the same age, but Dana is older and as we could see from the partial dependency plot and feature importance plot, the older you are the higher your wage is, up untill 70 years old. Also years of experience plays a big role, Dana once again has the most followed by Rahel and with no experience we have Ian and Piet. For industry we have all said that we will work in the Computers/Technology industry, and male's get paid higher in this industry, boosting the wages of Ian and Piet just a small bit. As gender doesn't play that big of a factor, you can see this from the feature importance plot. On the other hand Country does play a big role, For Rahel and Dana, they plan to work in Switzelrand which has a very strong economy with an average pay that's way higher than Belgium, Ian is planning on working in Denmark which lays somewhere in between Switzerland and Belgium. Piet filled everything in as he was working full time except for job_role, here he said that he would be a student which severely impacted his wage as, job_role being student is high up the variable importance plot. This would explain why his pay would be relatively little compared to the others. Ian and Rahel are also kind of similar, although Rahel would work in Switzerland which should get paid more, Ian has more experience when it comes to Machine learning, and his job would be more coding focused. Resulting in a higher pay. Aslo Rahel's job_role is Other as the job which she would be doing is not in the list, if her specific job would be in the list she might end up with a higher wage than Ian. The same goes for Dan, her job_role is currently also Other, if her specific role was in the list to choose from she might end up with an even higher wage. Than you might say are Dana and Rahel not very similar and shouldn't they get payed about the same, as they are both female, in Switzerland, with the job_role other? No, there is still a more than 30.000 difference which could be due to like mentioned before, experience but also Dana has had between 5-10 years of writing code to analyze data.

# 14 Save Model and Results

Optionally, you can persist your model and predictions for deployment or future reuse. As retraining the model takes a long time.

```{r save-results, eval=FALSE}
save(rf_model, GBM_rsme, best_model, file = "wage_model.RData")
write.csv(team_matrix, file = "team_wage_predictions.csv", row.names = FALSE)
```

# End of Report
