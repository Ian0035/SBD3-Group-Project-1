---
title: "Wage Prediction Using Machine Learning and AutoML"
author: "Feuz Dana Livia, Hoogstrate Ian, Kuchen Rahel, Vandecruys Piet"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# 1. Load Required Libraries

We start by loading in the necessary libraries. These libraries are for data exploration machine learning autoML and some require extra installations. For autoML you need to have the 64 bit version of JAVA.
```{r libraries}
# Install missing packages
packages <- c("caret", "randomForest", "pdp", "ggplot2", "dplyr", "caretEnsemble", "h2o", "data.table")
to_install <- packages[!packages %in% installed.packages()[, "Package"]]
if(length(to_install)) install.packages(to_install)


# Install dependencies
install.packages("RCurl")
install.packages("jsonlite")


install.packages("C:/Users/ianho/Downloads/h2o_3.44.0.3.zip", repos = NULL, type = "win.binary")


# Load libraries
library(h2o)
library(caret)
library(caretEnsemble)
library(randomForest)
library(pdp)
library(ggplot2)
library(dplyr)
library(data.table)
```

# 2. Load the Data

We load a preprocessed dataset data_wage.RData which contains the wages and all the features that we will need for the prediction.
```{r data-load}

load("data_wage.RData")
df <- data
```

# 3. Data Preprocessing

Goal: Understand the data before modeling.
Why? Data-driven decisions start with exploration.

We will first take a look at our data, we can see that our data exists out of more than 10 thousand rows and 78 features. We then check for missing values, and we can see that there are none, if there were any missing values, we would remove the row.
```{r data-explore}
str(df)
summary(df)

# Missing data summary
missing_summary <- sapply(df, function(x) sum(is.na(x)))
print(missing_summary)

# Decision: Drop rows with missing data if the proportion is low.
cat("Rows before NA removal:", nrow(df), "\n")
df <- na.omit(df)
cat("Rows after NA removal:", nrow(df), "\n")
```

# 4 Data exploration
Why? To justify feature selection based on data relationships.


When exploring the data, some weird data arrised, 18-21 year olds earining extremely high amounts? Are these outliers? Should we remove them? Let's take a look, first we take everyone that earns, while the features people ussually look at would dispove that Like for example, in our case is it believable that somebody that is 18-21 or for somebody that has less that 3 years of experience to earn more than 100 thousand? Probably not. So that what we took a look at. We gave everybody that earns more than 100k and that has less than 3 years of expereince or that is younger than 25 a possible outlier flag. We did this so it was easier to keep track of these individuals.

```{r data-explore}
# Outlier flags
df$outlier_flag <- with(df, 
                        (wage > 100000 & (years_experience %in% c("0-1", "1-2") | age %in% c("18-21", "22-24")))
)
```
First we thought it might had something to do with if they used ML at work. The easiest way to see if my hypothesis is right, is to disprove it. So we gave everyone that uses ML at work earns less than 50 thousand and is younger than 25 or has less than 3 years of experience a flag to disprove my hypothesis. Now if there was almost nobody that had this new flag then we could conclude that ML at work does in fact have something to do with it.
```{r data-explore}

df$disporve_outlier_flag <- with(df, 
                                 (wage < 50000 & (years_experience %in% c("0-1", "1-2") & age %in% c("18-21", "22-24") & ML_atwork %in% c("We have well established ML methods (i.e., models in production for more than 2 years)","We recently started using ML methods (i.e., models in production for less than 2 years)")))
)

cat("Low wage/low exp/ML:", sum(df$disporve_outlier_flag), "\n")
```

Sadly this wasn't the case, so we hadn't found a reason yet why some of these very young or inexperienced people earned so much. Time to do some further analysis. There are 78 features and over 10 thousand rows. With just looking at the data will be hard to find these features that are the cause of these high earners, that's why we will plot some charts to get a clear look of what these high earners do, or in what category they fall. We will plot charts for every categorical feature. 

```{r data-explore}
df_18_21 <- subset(df, age == "18-21")
df_18_21$income_group <- ifelse(df_18_21$wage >= 50000, "50k+", "<50k")
df_18_21$income_group <- factor(df_18_21$income_group, levels = c("<50k", "50k+"))
features_to_plot <- names(df_18_21)[!(names(df_18_21) %in% c("wage", "income_group", "outlier_flag", "disporve_outlier_flag"))]
cat_features <- features_to_plot[sapply(df_18_21[features_to_plot], function(x) is.character(x) || is.factor(x))]

# Loop through and plot each categorical feature
for (feature in cat_features) {
  p <- ggplot(df_18_21, aes_string(x = feature, fill = "income_group")) +
    geom_bar(position = "fill") +
    scale_y_continuous(labels = scales::percent_format()) +
    labs(title = paste("Income Group Proportions by", feature),
         x = feature,
         y = "Proportion",
         fill = "Income Group") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(p)
}



```

These plots are from the goFrom this we could see some interesting statistics, just check out the plots, percent actively coding, How long have you been writing code to analyze data, and for how many years have you used machine learning methods at work or in school. Other graphs like the one "Do you consider yourself to be a data scientist" will not help us at all, as every column has around the same amount of people who earn a lot, making this noise across all categories for that feature.


A normal question is should we consider gender as an important feature? While gender used to heavily affect the wage, does it still to this day and should we include this? Is this ethical?

```{r is gender an important feature?}
ggplot(df, aes(x = gender, y = wage)) +
  geom_boxplot(fill = "lightblue", outlier.shape = NA) +
  coord_cartesian(ylim = c(0, quantile(df$wage, 0.95))) +  # optional: cap extreme outliers
  labs(
    title = "Wage Distribution by Gender Across Industries",
    y = "Wage",
    x = "Gender"
  ) +
  facet_wrap(~ industry, scales = "free_y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```
Gender includes Female, Male, Prefer not to say, and Prefer to self-describe. The difference between the median of all these is very minor, so you might think we could let this variable out. But if we look closer, per industry for example, we see that females on average earn a lot more than males, if we would let the feature gender go and we would try to predict a female or male for the industry broadcasting, it would give a very wrong answer. So even though it might not be ethical, we do have to leave the feature in.


```{r is gender an important feature?}
# Numeric correlation with wage
numeric_vars <- sapply(df, is.numeric)
if (sum(numeric_vars) > 1) {
  cor_wage <- cor(df[, numeric_vars], use = "complete.obs")
  print(cor_wage["wage", ])
}

# Group means for categorical features
cat_vars <- c("education", "years_experience", "gender", "country", "job_role", "industry", "age")
for (v in cat_vars) {
  print(ggplot(df, aes_string(x = v, y = "wage")) +
          geom_boxplot() +
          ggtitle(paste("Wage by", v)))
}

# Statistical test: Does education level affect wage?
anova_education <- aov(wage ~ education, data = df)
cat("ANOVA for wage by education:\n")
print(summary(anova_education))

# Use these EDA results to select variables showing clear wage differences.

```
# 5 Data preparation & encoding

Convert categorical variables to factors:

Categorical variables are converted to factors to prepare them for modeling.
```{r factor-conversion}
df$gender <- as.factor(df$gender)
df$education <- as.factor(df$education)
df$country <- as.factor(df$country)
df$age <- as.factor(df$age)
df$years_experience <- as.factor(df$years_experience)
df$job_role <- as.factor(df$job_role)
df$industry <- as.factor(df$industry)
df$For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school.. <- as.factor(df$For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school..)
df$How.long.have.you.been.writing.code.to.analyze.data. <- as.factor(df$How.long.have.you.been.writing.code.to.analyze.data.)
```

# Feature Selection

We select key predictors and that we think are important for our model to predict the right wage.

```{r Feature Selection}
model_data <- df %>% 
  select(wage, age, years_experience, education, gender, country, job_role, industry, How.long.have.you.been.writing.code.to.analyze.data., For.how.many.years.have.you.used.machine.learning.methods..at.work.or.in.school..)

```

# Dummy Encoding

We apply dummy encoding to convert categorical features to numeric format for autoML modeling.

```{r dummy-encoding}

dummy_model <- caret::dummyVars(~ ., data = model_data[,-1])
dummy_data <- predict(dummy_model, newdata = model_data[,-1])
model_matrix <- data.frame(wage = model_data$wage, dummy_data)
```

# 6. Train-Test Split

We split the data into training and testing sets to evaluate model generalization.

```{r train-test-split}
set.seed(123)
train_index <- createDataPartition(model_matrix$wage, p = 0.7, list = FALSE)
train_data <- model_matrix[train_index, ]
test_data  <- model_matrix[-train_index, ]
```

# 7. AutoML with H2O

We use H2O’s AutoML to automatically train and tune multiple models. XGBoost is excluded due to compatibility issues with windows.

```{r automl}
h2o.init()

h2o.xgboost.available()

df_h2o <- as.h2o(model_matrix)
set.seed(12)
splits <- h2o.splitFrame(df_h2o, ratios = 0.8, seed = 1234)
train <- splits[[1]]
valid <- splits[[2]]

dep_var <- "wage"
indep_vars <- setdiff(colnames(df_h2o), dep_var)

automl <- h2o.automl(
  x = indep_vars,
  y = dep_var,
  training_frame = train,
  leaderboard_frame = valid,
  max_models = 10,
  seed = 12,
  sort_metric = "RMSE",
  exclude_algos = c("XGBoost")
)

lb <- automl@leaderboard
print(lb)

best_model <- automl@leader
print(best_model)
```
# 8. Random Forest Model Development

We build a traditional Random Forest for comparison.

```{r rf-model}
set.seed(123)
rf_model <- randomForest(wage ~ ., data = train_data, importance = TRUE)
print(rf_model)
```


# 9 AutoML Explainability

We use H2O’s built-in tools to explore variable importance and local explanations.

```{r automl-explain}
exp_automl <- h2o.explain(automl, valid)
print(exp_automl)

StackedEnsemble_rsme <- h2o.get_best_model(automl, algorithm = "StackedEnsemble", criterion = "rmse")
GBM_rsme <- h2o.get_best_model(automl, algorithm = "GBM", criterion = "rmse")

exp_GBM <- h2o.explain(GBM_rsme, valid)
print(exp_GBM)

h2o.varimp_plot(GBM_rsme)
```

# 10 Random Forest Explainability

We visualize feature importance and partial dependence of wages on experience.

```{r rf-explain}

# For Random Forest
importance_values <- importance(rf_model)
print(importance_values)
varImpPlot(rf_model, main = "Variable Importance for Wage Prediction (RF)")
cat("Top 5 RF features:\n")
print(head(sort(importance(rf_model)[,1], decreasing = TRUE), 5))

# Partial dependence plot for years_experience
pdp_experience <- partial(rf_model, pred.var = "years_experience", train = train_data)
plotPartial(pdp_experience, main = "Partial Dependence: Wage ~ Years of Experience",
            xlab = "Years of Experience", ylab = "Predicted Wage")

# For H2O AutoML: Explain GBM (often one of the top models)
GBM_rsme <- h2o.get_best_model(automl, algorithm = "GBM", criterion = "rmse")
h2o.varimp_plot(GBM_rsme)


single_best_rmse <- performance_single(perf_best_rmse)

# Extract predictions and actuals
pred_df <- as.data.frame(h2o.cbind(pred_best_rmse, valid$wage))
colnames(pred_df) <- c("predicted", "actual")

```


# 11 AutoML VS our own Random Forest model

```{r rf-eval}
# Performance of AutoML model
perf_best_rmse <- h2o.performance(best_model, valid)
aml_rmse <- h2o.rmse(perf_best_rmse)
aml_mae  <- h2o.mae(perf_best_rmse)
aml_r2   <- h2o.r2(perf_best_rmse)

# Performance of Random Forest (train/test split)
rf_model <- randomForest(wage ~ ., data = train_data, importance = TRUE)
rf_pred <- predict(rf_model, test_data)
rf_rmse <- sqrt(mean((test_data$wage - rf_pred)^2))
rf_mae  <- mean(abs(test_data$wage - rf_pred))
rf_r2   <- cor(test_data$wage, rf_pred)^2

# Results summary table
results_table <- data.frame(
  Model = c("Random Forest", "H2O AutoML"),
  RMSE = c(rf_rmse, aml_rmse),
  MAE  = c(rf_mae, aml_mae),
  R2   = c(rf_r2, aml_r2)
)
print(results_table)

best_rmse <- h2o.get_best_model(automl, criterion = "RMSE")
pred_best_rmse <- h2o.predict(best_rmse, valid)
predictions <- as.data.table(pred_best_rmse)
perf_best_rmse <- h2o.performance(best_rmse, valid)


```

# 12 Plot residuals and predicted vs actual values:

We visualize how well the model predicted wages using residual and prediction plots.

```{r automl-plots, fig.width=6, fig.height=4}
# AutoML predictions
pred_best_rmse <- h2o.predict(best_model, valid)
pred_df <- as.data.frame(h2o.cbind(pred_best_rmse, valid$wage))
colnames(pred_df) <- c("predicted", "actual")

# Residuals plot
ggplot(pred_df, aes(x = actual, y = predicted - actual)) +
  geom_point(alpha = 0.4) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residuals vs. Actual Wage", x = "Actual Wage", y = "Residuals")

# Predicted vs actual
ggplot(pred_df, aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, color = "blue", linetype = "dashed") +
  labs(title = "Predicted vs. Actual Wage", x = "Actual Wage", y = "Predicted Wage")

```


# 13 Real-world Application: Predicting Team Member Wages

We use the trained model to predict the wages of team members based on their profile.

```{r predict-team}
team_raw <- data.frame(
  age = factor(c("30-34", "35-39", "22-24"), levels = levels(df$age)),
  years_experience = factor(c("5-11", "5-11", "0-1"), levels = levels(df$years_experience)),
  education = factor(c("Master’s degree", "Doctoral degree", "Bachelor’s degree"), levels = levels(df$education)),
  gender = factor(c("Female", "Male", "Male"), levels = levels(df$gender)),
  country = factor(c("United States of America", "United States of America", "Switzerland"), 
                   levels = levels(df$country)),
  job_role = factor(c("Data Scientist", "Software Engineer", "Student"), levels = levels(df$job_role)),
  industry = factor(c("Computers/Technology", "Computers/Technology", "I am a student"), levels = levels(df$industry))
)

team_dummy <- predict(dummy_model, newdata = team_raw)
team_matrix <- data.frame(team_raw, team_dummy)
team_matrix$predicted_wage <- predict(rf_model, newdata = team_matrix)

team_matrix[, c("age", "years_experience", "education", "gender", "country", "job_role", "industry", "predicted_wage")]
```

# 14 Save Model and Results

Optionally, you can persist your model and predictions for deployment or future reuse. As retraining the model takes a long time.

```{r save-results, eval=FALSE}
save(rf_model, file = "rf_wage_model.RData")
write.csv(team_matrix, file = "team_wage_predictions.csv", row.names = FALSE)
```

# End of Report
